{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFvFhIaGuZjc",
        "outputId": "21bab94f-b936-4084-8d5b-4acebc741919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\OneDrive\\Documents\\ML Train\\UNAL\\Unidad 6\\MLDS6_Proyecto\\scripts\\training\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3Pt_H7lsqkT",
        "outputId": "4ebe5581-2361-437a-f761-a97942434fef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 26709 entries, 0 to 26708\n",
            "Data columns (total 4 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   article_link  26709 non-null  object\n",
            " 1   headline      26709 non-null  object\n",
            " 2   is_sarcastic  26709 non-null  int64 \n",
            " 3   clean_text    26709 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 834.8+ KB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "!ls\n",
        "# Load data\n",
        "data = pd.read_json('../../src/bazinga/database/Clean_Headlines.json', lines=True)\n",
        "headlines = data['headline']\n",
        "labels = data['is_sarcastic']\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tatk4KwAJCq",
        "outputId": "c2d020ca-b557-4598-fa7d-abf3db01120d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cJabzgxY0-R8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "349120b86c46480f80c9c88f6e6f3f46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21784992e1954b4d9ff0291e07bb2e2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5509fe4770c447ce9ec739ef250feaa4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61eac053a2844a6e9d805de44d27fc59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83e1f2ceffae473fada03839f2d72362",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "def get_bert_embeddings(texts, batch_size=32):\n",
        "    \n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Use [CLS] token and move back to CPU\n",
        "        embeddings.append(batch_embeddings)\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "bert_embeddings = get_bert_embeddings(headlines.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "T66tIf4w7f4k"
      },
      "outputs": [],
      "source": [
        "# Save BERT embeddings to file\n",
        "np.save('bert_embeddings.npy', bert_embeddings)\n",
        "bert_df = pd.DataFrame(bert_embeddings)\n",
        "bert_df.to_csv('bert_embeddings.csv', index=False)\n",
        "\n",
        "# Load BERT embeddings from file\n",
        "loaded_bert_embeddings = np.load('bert_embeddings.npy')\n",
        "loaded_bert_df = pd.read_csv('bert_embeddings.csv')\n",
        "loaded_bert_embeddings_from_csv = loaded_bert_df.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC9_TNur1UzW",
        "outputId": "1bfc69ce-03b1-45d9-85b6-3a5ee3f14b4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.27875116  0.12506711 -0.43516022 ... -0.26109472  0.1895398\n",
            "   0.30300733]\n",
            " [ 0.28687197  0.1983599   0.06834903 ... -0.2625548   0.18669029\n",
            "   0.46911165]\n",
            " [-0.54032004 -0.2303586   0.4740718  ... -0.4492011   0.36399528\n",
            "   0.01749159]\n",
            " ...\n",
            " [-0.2593226  -0.09551121 -0.61755043 ... -0.05595841  0.46902105\n",
            "   0.20650041]\n",
            " [-0.09609874  0.23279017 -0.24334535 ... -0.47870547 -0.01567071\n",
            "   0.1109514 ]\n",
            " [-0.25170833  0.01995288  0.24350114 ... -0.31329003 -0.02248639\n",
            "   0.32390368]]\n"
          ]
        }
      ],
      "source": [
        "print(bert_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vs4dTHKNrJj6"
      },
      "outputs": [],
      "source": [
        "# Tokenize the headlines\n",
        "tokenized_headlines = [headline.split() for headline in headlines]\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=tokenized_headlines, vector_size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Generate Word2Vec embeddings\n",
        "def get_w2v_embeddings(texts, model):\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "        if word_vectors:\n",
        "            embeddings.append(np.mean(word_vectors, axis=0))\n",
        "        else:\n",
        "            embeddings.append(np.zeros(300))  # Handle empty cases\n",
        "    return np.array(embeddings)\n",
        "\n",
        "w2v_embeddings = get_w2v_embeddings(headlines, w2v_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "svEHaJ9DBK21"
      },
      "outputs": [],
      "source": [
        "# Save Word2Vec embeddings\n",
        "np.save('w2v_embeddings.npy', w2v_embeddings)\n",
        "w2v_df = pd.DataFrame(w2v_embeddings)\n",
        "\n",
        "# Save to CSV\n",
        "w2v_df.to_csv('w2v_embeddings.csv', index=False)\n",
        "loaded_w2v_embeddings = np.load('w2v_embeddings.npy')\n",
        "loaded_w2v_df = pd.read_csv('w2v_embeddings.csv')\n",
        "loaded_w2v_embeddings = loaded_w2v_df.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5788skmz84F",
        "outputId": "8ea278d6-271e-4e1e-9e9b-11018565c789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.02992626  0.21308674  0.03776343 ... -0.14672764  0.24013734\n",
            "  -0.04816075]\n",
            " [-0.04379579  0.38479638  0.07330603 ... -0.2341625   0.40016124\n",
            "  -0.09233301]\n",
            " [-0.03820257  0.37990794  0.07382133 ... -0.20238481  0.37978002\n",
            "  -0.09466912]\n",
            " ...\n",
            " [-0.06332782  0.4649342   0.08560308 ... -0.3022044   0.50195813\n",
            "  -0.10515881]\n",
            " [-0.01427451  0.08703323  0.01569336 ... -0.0582545   0.09946428\n",
            "  -0.02333586]\n",
            " [-0.03153266  0.3034843   0.05635116 ... -0.19201021  0.31785488\n",
            "  -0.07332972]]\n"
          ]
        }
      ],
      "source": [
        "print(w2v_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
